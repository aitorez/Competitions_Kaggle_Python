{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descripción del conjunto de datos**\n",
    "\n",
    "El conjunto de datos para esta competencia (tanto de entrenamiento como de prueba) se generó a partir de un modelo de aprendizaje profundo entrenado en Credit Card Fraud Detection . Las distribuciones de características son similares, pero no exactamente iguales, a las del original. Siéntase libre de usar el conjunto de datos original como parte de esta competencia, tanto para explorar las diferencias como para ver si incorporar el original en el entrenamiento mejora el rendimiento del modelo.\n",
    "\n",
    "Tenga en cuenta que este conjunto de datos base para esta competencia era mucho más grande que los conjuntos de datos anteriores de Tabular Tuesday y, por lo tanto, puede contener más artefactos que las últimas tres competencias.\n",
    "\n",
    "**Archivos:**\n",
    "\n",
    "    train.csv - el conjunto de datos de entrenamiento; Classes el objetivo\n",
    "    test.csv - el conjunto de datos de prueba; tu objetivo es predecirClass\n",
    "    sample_submission.csv : un archivo de envío de muestra en el formato correcto\n",
    "\n",
    "\n",
    "**Documentación:**\n",
    "\n",
    "El conjunto de datos contiene transacciones realizadas con tarjetas de crédito en septiembre de 2013 por titulares de tarjetas europeos.\n",
    "Este conjunto de datos presenta transacciones que ocurrieron en dos días, donde tenemos 492 fraudes de 284,807 transacciones. El conjunto de datos está muy desequilibrado, la clase positiva (fraudes) representa el 0,172 % de todas las transacciones.\n",
    "\n",
    "Contiene solo variables de entrada numéricas que son el resultado de una transformación PCA. Desafortunadamente, debido a problemas de confidencialidad, no podemos proporcionar las funciones originales ni más información general sobre los datos. Las características V1, V2, … V28 son los principales componentes obtenidos con PCA, las únicas características que no han sido transformadas con PCA son 'Tiempo' y 'Cantidad'. La característica 'Tiempo' contiene los segundos transcurridos entre cada transacción y la primera transacción en el conjunto de datos. La función 'Cantidad' es la cantidad de la transacción, esta función se puede utilizar para el aprendizaje sensible a los costos dependiente del ejemplo. Feature 'Class' es la variable de respuesta y toma valor 1 en caso de fraude y 0 en caso contrario.\n",
    "\n",
    "Dada la relación de desequilibrio de clase, recomendamos medir la precisión utilizando el área bajo la curva de recuperación de precisión (AUPRC). La precisión de la matriz de confusión no es significativa para la clasificación desequilibrada.\n",
    "\n",
    "\n",
    "**Evaluación:**\n",
    "\n",
    "Las presentaciones se evalúan en el área bajo la curva ROC entre la probabilidad pronosticada y el objetivo observado.\n",
    "\n",
    "Para cada uno iden el conjunto de prueba, debe predecir el valor del objetivo Class. El archivo debe contener un encabezado y tener el siguiente formato:\n",
    "\n",
    "    id,Class\n",
    "    341588,0.23\n",
    "    341589,0.92\n",
    "    341590,0.02\n",
    "    etc.\n",
    "\n",
    "**REFERENCIAS:**\n",
    "\n",
    "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson y Gianluca Bontempi. Calibración de probabilidad con submuestreo para clasificación desequilibrada. En Simposio sobre Inteligencia Computacional y Minería de Datos (CIDM), IEEE, 2015\n",
    "\n",
    "Dal Pozolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Lecciones aprendidas en la detección de fraudes con tarjetas de crédito desde la perspectiva de un profesional , Sistemas expertos con aplicaciones, 41,10,4915-4928,2014, Pergamon\n",
    "\n",
    "Dal Pozolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, César; Bontempi, Gianluca. Detección de fraude con tarjetas de crédito: un modelo realista y una estrategia de aprendizaje novedosa, transacciones IEEE en redes neuronales y sistemas de aprendizaje, 29,8,3784-3797,2018, IEEE\n",
    "\n",
    "Dal Pozzolo, Andrea Adaptive Machine learning para la detección de fraudes con tarjetas de crédito Tesis doctoral de ULB MLG (supervisada por G. Bontempi)\n",
    "\n",
    "Carcillo, Fabricio; Dal Pozolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: un marco escalable para transmitir la detección de fraudes con tarjetas de crédito con Spark , Information fusion,41, 182-194,2018,Elsevier\n",
    "\n",
    "Carcillo, Fabricio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Transmisión de estrategias de aprendizaje activo para la detección de fraudes con tarjetas de crédito en la vida real: evaluación y visualización, International Journal of Data Science and Analytics, 5,4,285-300,2018, Springer International Publishing\n",
    "\n",
    "Bertrand Lebichot, Yann-Aël Le Borgne, Liyun He, Frederic Oblé, Gianluca Bontempi Técnicas de adaptación de dominios de aprendizaje profundo para la detección de fraudes con tarjetas de crédito , INNSBDDL 2019: Avances recientes en Big Data y aprendizaje profundo, págs. 78-88, 2019\n",
    "\n",
    "Fabrizio Carcillo, Yann-Aël Le Borgne, Olivier Caelen, Frederic Oblé, Gianluca Bontempi Combinación de aprendizaje no supervisado y supervisado en ciencias de la información de detección de fraude con tarjetas de crédito , 2019\n",
    "\n",
    "Yann-Aël Le Borgne, Gianluca Bontempi Aprendizaje automático reproducible para la detección de fraudes con tarjetas de crédito - Manual práctico\n",
    "\n",
    "Bertrand Lebichot, Gianmarco Paldino, Wissam Siblini, Liyun He, Frederic Oblé, Gianluca Bontempi Estrategias de aprendizaje incremental para la detección de fraudes con tarjetas de crédito , International Journal of Data Science and Analytics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'/Users/aitorelordizamora/Library/Mobile Documents/com~apple~CloudDocs/Datasets/datasets fraude/train.csv', sep=',')\n",
    "df_test = pd.read_csv(r'/Users/aitorelordizamora/Library/Mobile Documents/com~apple~CloudDocs/Datasets/datasets fraude/test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.074329</td>\n",
       "      <td>-0.129425</td>\n",
       "      <td>-1.137418</td>\n",
       "      <td>0.412846</td>\n",
       "      <td>-0.192638</td>\n",
       "      <td>-1.210144</td>\n",
       "      <td>0.110697</td>\n",
       "      <td>-0.263477</td>\n",
       "      <td>0.742144</td>\n",
       "      <td>0.108782</td>\n",
       "      <td>-1.070243</td>\n",
       "      <td>-0.234910</td>\n",
       "      <td>-1.099360</td>\n",
       "      <td>0.502467</td>\n",
       "      <td>0.169318</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>-0.306957</td>\n",
       "      <td>-0.323800</td>\n",
       "      <td>0.103348</td>\n",
       "      <td>-0.292969</td>\n",
       "      <td>-0.334701</td>\n",
       "      <td>-0.887840</td>\n",
       "      <td>0.336701</td>\n",
       "      <td>-0.110835</td>\n",
       "      <td>-0.291459</td>\n",
       "      <td>0.207733</td>\n",
       "      <td>-0.076576</td>\n",
       "      <td>-0.059577</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.998827</td>\n",
       "      <td>-1.250891</td>\n",
       "      <td>-0.520969</td>\n",
       "      <td>-0.894539</td>\n",
       "      <td>-1.122528</td>\n",
       "      <td>-0.270866</td>\n",
       "      <td>-1.029289</td>\n",
       "      <td>0.050198</td>\n",
       "      <td>-0.109948</td>\n",
       "      <td>0.908773</td>\n",
       "      <td>0.836798</td>\n",
       "      <td>-0.056580</td>\n",
       "      <td>-0.120990</td>\n",
       "      <td>-0.144028</td>\n",
       "      <td>-0.039582</td>\n",
       "      <td>1.653057</td>\n",
       "      <td>-0.253599</td>\n",
       "      <td>-0.814354</td>\n",
       "      <td>0.716784</td>\n",
       "      <td>0.065717</td>\n",
       "      <td>0.054848</td>\n",
       "      <td>-0.038367</td>\n",
       "      <td>0.133518</td>\n",
       "      <td>-0.461928</td>\n",
       "      <td>-0.465491</td>\n",
       "      <td>-0.464655</td>\n",
       "      <td>-0.009413</td>\n",
       "      <td>-0.038238</td>\n",
       "      <td>84.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.091535</td>\n",
       "      <td>1.004517</td>\n",
       "      <td>-0.223445</td>\n",
       "      <td>-0.435249</td>\n",
       "      <td>0.667548</td>\n",
       "      <td>-0.988351</td>\n",
       "      <td>0.948146</td>\n",
       "      <td>-0.084789</td>\n",
       "      <td>-0.042027</td>\n",
       "      <td>-0.818383</td>\n",
       "      <td>-0.376512</td>\n",
       "      <td>-0.226546</td>\n",
       "      <td>-0.552869</td>\n",
       "      <td>-0.886466</td>\n",
       "      <td>-0.180890</td>\n",
       "      <td>0.230286</td>\n",
       "      <td>0.590579</td>\n",
       "      <td>-0.321590</td>\n",
       "      <td>-0.433959</td>\n",
       "      <td>-0.021375</td>\n",
       "      <td>-0.326725</td>\n",
       "      <td>-0.803736</td>\n",
       "      <td>0.154495</td>\n",
       "      <td>0.951233</td>\n",
       "      <td>-0.506919</td>\n",
       "      <td>0.085046</td>\n",
       "      <td>0.224458</td>\n",
       "      <td>0.087356</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.979649</td>\n",
       "      <td>-0.184949</td>\n",
       "      <td>-1.064206</td>\n",
       "      <td>0.120125</td>\n",
       "      <td>-0.215238</td>\n",
       "      <td>-0.648829</td>\n",
       "      <td>-0.087826</td>\n",
       "      <td>-0.035367</td>\n",
       "      <td>0.885838</td>\n",
       "      <td>-0.007527</td>\n",
       "      <td>0.637441</td>\n",
       "      <td>0.676960</td>\n",
       "      <td>-1.504823</td>\n",
       "      <td>0.554039</td>\n",
       "      <td>-0.824356</td>\n",
       "      <td>-0.527267</td>\n",
       "      <td>-0.095838</td>\n",
       "      <td>-0.312519</td>\n",
       "      <td>0.642659</td>\n",
       "      <td>-0.340089</td>\n",
       "      <td>-0.095514</td>\n",
       "      <td>-0.079792</td>\n",
       "      <td>0.167701</td>\n",
       "      <td>-0.042939</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>-0.096148</td>\n",
       "      <td>-0.057780</td>\n",
       "      <td>-0.073839</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.025898</td>\n",
       "      <td>-0.171827</td>\n",
       "      <td>1.203717</td>\n",
       "      <td>1.243900</td>\n",
       "      <td>-0.636572</td>\n",
       "      <td>1.099074</td>\n",
       "      <td>-0.938651</td>\n",
       "      <td>0.569239</td>\n",
       "      <td>0.692665</td>\n",
       "      <td>-0.097495</td>\n",
       "      <td>1.338869</td>\n",
       "      <td>1.391399</td>\n",
       "      <td>-0.128167</td>\n",
       "      <td>-0.081836</td>\n",
       "      <td>0.100548</td>\n",
       "      <td>-0.338937</td>\n",
       "      <td>0.090864</td>\n",
       "      <td>-0.423645</td>\n",
       "      <td>-0.731939</td>\n",
       "      <td>-0.203628</td>\n",
       "      <td>0.099157</td>\n",
       "      <td>0.608908</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>-0.262813</td>\n",
       "      <td>0.257834</td>\n",
       "      <td>-0.252829</td>\n",
       "      <td>0.108338</td>\n",
       "      <td>0.021051</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219124</th>\n",
       "      <td>219124</td>\n",
       "      <td>120580.0</td>\n",
       "      <td>1.891079</td>\n",
       "      <td>-1.272908</td>\n",
       "      <td>-3.783908</td>\n",
       "      <td>-1.388939</td>\n",
       "      <td>2.012789</td>\n",
       "      <td>2.666080</td>\n",
       "      <td>0.151740</td>\n",
       "      <td>0.401934</td>\n",
       "      <td>-1.102824</td>\n",
       "      <td>0.858158</td>\n",
       "      <td>-0.280639</td>\n",
       "      <td>0.007976</td>\n",
       "      <td>-0.124950</td>\n",
       "      <td>0.914374</td>\n",
       "      <td>-0.073169</td>\n",
       "      <td>-2.309929</td>\n",
       "      <td>-0.041423</td>\n",
       "      <td>1.190526</td>\n",
       "      <td>-0.281848</td>\n",
       "      <td>-0.195703</td>\n",
       "      <td>-0.181369</td>\n",
       "      <td>-0.456538</td>\n",
       "      <td>-0.069571</td>\n",
       "      <td>0.756765</td>\n",
       "      <td>0.244479</td>\n",
       "      <td>-0.147566</td>\n",
       "      <td>-0.054725</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>198.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219125</th>\n",
       "      <td>219125</td>\n",
       "      <td>120580.0</td>\n",
       "      <td>0.139724</td>\n",
       "      <td>0.948649</td>\n",
       "      <td>-2.913655</td>\n",
       "      <td>-2.184829</td>\n",
       "      <td>1.883716</td>\n",
       "      <td>-1.056824</td>\n",
       "      <td>1.725624</td>\n",
       "      <td>0.018089</td>\n",
       "      <td>-0.823494</td>\n",
       "      <td>-0.257933</td>\n",
       "      <td>-0.457534</td>\n",
       "      <td>0.516146</td>\n",
       "      <td>-0.071240</td>\n",
       "      <td>1.310799</td>\n",
       "      <td>-1.892909</td>\n",
       "      <td>-0.318780</td>\n",
       "      <td>-0.917395</td>\n",
       "      <td>0.098397</td>\n",
       "      <td>-0.195558</td>\n",
       "      <td>-0.116538</td>\n",
       "      <td>0.491469</td>\n",
       "      <td>1.478823</td>\n",
       "      <td>-0.085398</td>\n",
       "      <td>-0.091409</td>\n",
       "      <td>-1.053488</td>\n",
       "      <td>0.467570</td>\n",
       "      <td>0.358918</td>\n",
       "      <td>0.294735</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219126</th>\n",
       "      <td>219126</td>\n",
       "      <td>120580.0</td>\n",
       "      <td>2.058343</td>\n",
       "      <td>-0.038993</td>\n",
       "      <td>-1.928553</td>\n",
       "      <td>0.330117</td>\n",
       "      <td>0.270127</td>\n",
       "      <td>-0.735664</td>\n",
       "      <td>-0.173878</td>\n",
       "      <td>0.144823</td>\n",
       "      <td>0.849289</td>\n",
       "      <td>-0.136498</td>\n",
       "      <td>0.179926</td>\n",
       "      <td>-1.769641</td>\n",
       "      <td>-3.937694</td>\n",
       "      <td>0.031346</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.883566</td>\n",
       "      <td>0.391801</td>\n",
       "      <td>1.007789</td>\n",
       "      <td>0.303376</td>\n",
       "      <td>-0.384830</td>\n",
       "      <td>-0.306640</td>\n",
       "      <td>-0.965783</td>\n",
       "      <td>0.307799</td>\n",
       "      <td>-0.021434</td>\n",
       "      <td>-0.343989</td>\n",
       "      <td>0.181065</td>\n",
       "      <td>-0.098387</td>\n",
       "      <td>-0.044064</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219127</th>\n",
       "      <td>219127</td>\n",
       "      <td>120580.0</td>\n",
       "      <td>2.079227</td>\n",
       "      <td>-2.162389</td>\n",
       "      <td>-1.785833</td>\n",
       "      <td>-2.804889</td>\n",
       "      <td>0.552845</td>\n",
       "      <td>4.038013</td>\n",
       "      <td>-2.155900</td>\n",
       "      <td>1.023785</td>\n",
       "      <td>-0.865242</td>\n",
       "      <td>1.536193</td>\n",
       "      <td>-0.058879</td>\n",
       "      <td>-0.885949</td>\n",
       "      <td>-0.254718</td>\n",
       "      <td>-0.425730</td>\n",
       "      <td>0.665556</td>\n",
       "      <td>-0.336634</td>\n",
       "      <td>0.301966</td>\n",
       "      <td>0.391249</td>\n",
       "      <td>0.037770</td>\n",
       "      <td>-0.190984</td>\n",
       "      <td>0.109909</td>\n",
       "      <td>0.590401</td>\n",
       "      <td>0.286621</td>\n",
       "      <td>0.675660</td>\n",
       "      <td>-0.510736</td>\n",
       "      <td>-0.090044</td>\n",
       "      <td>0.056749</td>\n",
       "      <td>-0.017126</td>\n",
       "      <td>88.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219128</th>\n",
       "      <td>219128</td>\n",
       "      <td>120580.0</td>\n",
       "      <td>-0.431758</td>\n",
       "      <td>1.299171</td>\n",
       "      <td>-0.571602</td>\n",
       "      <td>-1.161499</td>\n",
       "      <td>1.141765</td>\n",
       "      <td>-1.258871</td>\n",
       "      <td>1.701553</td>\n",
       "      <td>-0.454580</td>\n",
       "      <td>-0.556978</td>\n",
       "      <td>-0.049454</td>\n",
       "      <td>1.374548</td>\n",
       "      <td>1.172926</td>\n",
       "      <td>0.722704</td>\n",
       "      <td>0.525669</td>\n",
       "      <td>-0.926260</td>\n",
       "      <td>-0.379812</td>\n",
       "      <td>-0.640176</td>\n",
       "      <td>-0.161739</td>\n",
       "      <td>0.034470</td>\n",
       "      <td>0.058133</td>\n",
       "      <td>0.225629</td>\n",
       "      <td>0.988442</td>\n",
       "      <td>-0.224609</td>\n",
       "      <td>0.082977</td>\n",
       "      <td>-0.335529</td>\n",
       "      <td>0.042237</td>\n",
       "      <td>0.304965</td>\n",
       "      <td>0.240049</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219129 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id      Time        V1        V2        V3        V4        V5  \\\n",
       "0            0       0.0  2.074329 -0.129425 -1.137418  0.412846 -0.192638   \n",
       "1            1       0.0  1.998827 -1.250891 -0.520969 -0.894539 -1.122528   \n",
       "2            2       0.0  0.091535  1.004517 -0.223445 -0.435249  0.667548   \n",
       "3            3       0.0  1.979649 -0.184949 -1.064206  0.120125 -0.215238   \n",
       "4            4       0.0  1.025898 -0.171827  1.203717  1.243900 -0.636572   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "219124  219124  120580.0  1.891079 -1.272908 -3.783908 -1.388939  2.012789   \n",
       "219125  219125  120580.0  0.139724  0.948649 -2.913655 -2.184829  1.883716   \n",
       "219126  219126  120580.0  2.058343 -0.038993 -1.928553  0.330117  0.270127   \n",
       "219127  219127  120580.0  2.079227 -2.162389 -1.785833 -2.804889  0.552845   \n",
       "219128  219128  120580.0 -0.431758  1.299171 -0.571602 -1.161499  1.141765   \n",
       "\n",
       "              V6        V7        V8        V9       V10       V11       V12  \\\n",
       "0      -1.210144  0.110697 -0.263477  0.742144  0.108782 -1.070243 -0.234910   \n",
       "1      -0.270866 -1.029289  0.050198 -0.109948  0.908773  0.836798 -0.056580   \n",
       "2      -0.988351  0.948146 -0.084789 -0.042027 -0.818383 -0.376512 -0.226546   \n",
       "3      -0.648829 -0.087826 -0.035367  0.885838 -0.007527  0.637441  0.676960   \n",
       "4       1.099074 -0.938651  0.569239  0.692665 -0.097495  1.338869  1.391399   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "219124  2.666080  0.151740  0.401934 -1.102824  0.858158 -0.280639  0.007976   \n",
       "219125 -1.056824  1.725624  0.018089 -0.823494 -0.257933 -0.457534  0.516146   \n",
       "219126 -0.735664 -0.173878  0.144823  0.849289 -0.136498  0.179926 -1.769641   \n",
       "219127  4.038013 -2.155900  1.023785 -0.865242  1.536193 -0.058879 -0.885949   \n",
       "219128 -1.258871  1.701553 -0.454580 -0.556978 -0.049454  1.374548  1.172926   \n",
       "\n",
       "             V13       V14       V15       V16       V17       V18       V19  \\\n",
       "0      -1.099360  0.502467  0.169318  0.065688 -0.306957 -0.323800  0.103348   \n",
       "1      -0.120990 -0.144028 -0.039582  1.653057 -0.253599 -0.814354  0.716784   \n",
       "2      -0.552869 -0.886466 -0.180890  0.230286  0.590579 -0.321590 -0.433959   \n",
       "3      -1.504823  0.554039 -0.824356 -0.527267 -0.095838 -0.312519  0.642659   \n",
       "4      -0.128167 -0.081836  0.100548 -0.338937  0.090864 -0.423645 -0.731939   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "219124 -0.124950  0.914374 -0.073169 -2.309929 -0.041423  1.190526 -0.281848   \n",
       "219125 -0.071240  1.310799 -1.892909 -0.318780 -0.917395  0.098397 -0.195558   \n",
       "219126 -3.937694  0.031346  0.028100  0.883566  0.391801  1.007789  0.303376   \n",
       "219127 -0.254718 -0.425730  0.665556 -0.336634  0.301966  0.391249  0.037770   \n",
       "219128  0.722704  0.525669 -0.926260 -0.379812 -0.640176 -0.161739  0.034470   \n",
       "\n",
       "             V20       V21       V22       V23       V24       V25       V26  \\\n",
       "0      -0.292969 -0.334701 -0.887840  0.336701 -0.110835 -0.291459  0.207733   \n",
       "1       0.065717  0.054848 -0.038367  0.133518 -0.461928 -0.465491 -0.464655   \n",
       "2      -0.021375 -0.326725 -0.803736  0.154495  0.951233 -0.506919  0.085046   \n",
       "3      -0.340089 -0.095514 -0.079792  0.167701 -0.042939  0.000799 -0.096148   \n",
       "4      -0.203628  0.099157  0.608908  0.027901 -0.262813  0.257834 -0.252829   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "219124 -0.195703 -0.181369 -0.456538 -0.069571  0.756765  0.244479 -0.147566   \n",
       "219125 -0.116538  0.491469  1.478823 -0.085398 -0.091409 -1.053488  0.467570   \n",
       "219126 -0.384830 -0.306640 -0.965783  0.307799 -0.021434 -0.343989  0.181065   \n",
       "219127 -0.190984  0.109909  0.590401  0.286621  0.675660 -0.510736 -0.090044   \n",
       "219128  0.058133  0.225629  0.988442 -0.224609  0.082977 -0.335529  0.042237   \n",
       "\n",
       "             V27       V28  Amount  Class  \n",
       "0      -0.076576 -0.059577    1.98      0  \n",
       "1      -0.009413 -0.038238   84.00      0  \n",
       "2       0.224458  0.087356    2.69      0  \n",
       "3      -0.057780 -0.073839    1.00      0  \n",
       "4       0.108338  0.021051    1.00      0  \n",
       "...          ...       ...     ...    ...  \n",
       "219124 -0.054725 -0.044588  198.65      0  \n",
       "219125  0.358918  0.294735   24.00      0  \n",
       "219126 -0.098387 -0.044064    1.79      0  \n",
       "219127  0.056749 -0.017126   88.00      0  \n",
       "219128  0.304965  0.240049    3.78      0  \n",
       "\n",
       "[219129 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALORES NULOS: False\n",
      "VALORES DUPLICADOS: False\n"
     ]
    }
   ],
   "source": [
    "# Control valores nulos\n",
    "\n",
    "value_null = df_train.isnull().any().any()\n",
    "print(\"VALORES NULOS:\",value_null)\n",
    "\n",
    "\n",
    "# Control valores duplicados\n",
    "\n",
    "value_dupli = df_train.duplicated().any()\n",
    "print(\"VALORES DUPLICADOS:\",value_dupli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Contar la cantidad de muestras por clase\n",
    "class_counts = df_train['Class'].value_counts()\n",
    "\n",
    "# Visualizar el balance de clases\n",
    "plt.bar(class_counts.index, class_counts.values)\n",
    "plt.xlabel(\"Clase\")\n",
    "plt.ylabel(\"Cantidad de Muestras\")\n",
    "plt.xticks(class_counts.index, ['No Fraude', 'Fraude'])\n",
    "plt.title(\"Balance de Clases\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Hay desbalanceo en la muestra? Si\n",
    "\n",
    "El desbalanceo en un problema de clasificación ocurre cuando las clases objetivo (etiquetas de salida) en el conjunto de datos no están representadas de manera equilibrada, es decir, una clase tiene muchas más muestras que las otras. En otras palabras, el número de muestras en una clase es significativamente mayor que el número de muestras en la otra(s) clase(s).\n",
    "\n",
    "En problemas de clasificación binaria, el desbalanceo se refiere específicamente a una proporción desigual entre las dos clases. Por ejemplo, en un problema de detección de fraudes en transacciones bancarias, la clase \"No Fraude\" puede tener un número mucho mayor de muestras que la clase \"Fraude\" porque los casos de fraude son menos comunes en comparación con las transacciones regulares. En este caso, habría un desbalanceo de clases.\n",
    "\n",
    "El desbalanceo puede ser problemático para el entrenamiento de modelos de clasificación, especialmente cuando se utiliza un algoritmo que no maneja automáticamente este desequilibrio. Algunas de las consecuencias del desbalanceo incluyen:\n",
    "\n",
    "\n",
    "**Sesgo del modelo:** El modelo tiende a predecir con mayor precisión la clase mayoritaria y muestra una menor precisión para la clase minoritaria. Esto puede dar lugar a un rendimiento deficiente en la detección de la clase minoritaria (la clase que nos interesa detectar).\n",
    "\n",
    "\n",
    "**Altas tasas de falsos negativos:** En el caso de detección de fraudes, por ejemplo, puede haber una alta tasa de falsos negativos, lo que significa que el modelo puede pasar por alto muchos casos de fraude y considerarlos como transacciones regulares.\n",
    "\n",
    "\n",
    "**Menor rendimiento en métricas de evaluación:** Las métricas de evaluación, como la precisión, el recall y el AUC-ROC, pueden ser engañosas en un conjunto de datos desbalanceado, ya que el rendimiento general puede ser alto debido a la clasificación correcta de la clase mayoritaria, pero el rendimiento en la clase minoritaria es bajo.\n",
    "\n",
    "\n",
    "Para abordar el problema del desbalanceo, se pueden aplicar técnicas de manejo de desbalanceo, como:\n",
    "\n",
    "\n",
    "**Sobremuestreo de la clase minoritaria:** Aumentar artificialmente la cantidad de muestras de la clase minoritaria mediante técnicas como el duplicado de muestras existentes o la generación de nuevas muestras sintéticas.\n",
    "\n",
    "\n",
    "**Submuestreo de la clase mayoritaria:** Reducir la cantidad de muestras de la clase mayoritaria para equilibrar la proporción entre las clases.\n",
    "\n",
    "\n",
    "**Técnicas de generación de muestras sintéticas:** Utilizar algoritmos de generación de muestras sintéticas, como SMOTE (Synthetic Minority Over-sampling Technique), para crear muestras sintéticas de la clase minoritaria.\n",
    "\n",
    "\n",
    "**Ensemble de modelos:** Utilizar técnicas de ensamble que den más peso o importancia a la clasificación de la clase minoritaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - AUPRC: 0.0055, Training Time: 0.05 minutes\n",
      "Gradient Boosting - AUPRC: 0.0070, Training Time: 13.62 minutes\n",
      "Decision Tree - AUPRC: 0.0036, Training Time: 0.73 minutes\n",
      "Gaussian Naive Bayes - AUPRC: 0.0099, Training Time: 0.00 minutes\n",
      "XGBoost - AUPRC: 0.0031, Training Time: 3.70 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Separar características y variable objetivo en el conjunto de entrenamiento\n",
    "X = df_train.drop(['id', 'Class'], axis=1)  # Características\n",
    "y = df_train['Class']  # Variable objetivo\n",
    "\n",
    "\n",
    "# Obtener solo las columnas resultantes del PCA\n",
    "X_pca = X[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n",
    "       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n",
    "       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']]\n",
    "\n",
    "# Restar las columnas del PCA del conjunto de características original para obtener las características originales\n",
    "X_original = X[['Time', 'Amount']]\n",
    "\n",
    "# Preprocesar las características originales con StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_original_scaled = scaler.fit_transform(X_original)\n",
    "\n",
    "# Concatenar las características originales escaladas con las columnas del PCA\n",
    "X_preprocesado = pd.concat([pd.DataFrame(X_original_scaled, columns=X_original.columns), X_pca], axis=1)\n",
    "\n",
    "# Dividir el conjunto de entrenamiento en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_preprocesado, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aplicar SMOTE para corregir el desbalanceo\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Crear y entrenar distintos clasificadores\n",
    "classifiers = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Ajustar el clasificador en el conjunto de entrenamiento balanceado\n",
    "    classifier.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Evaluar el clasificador en el conjunto de validación\n",
    "    y_pred = classifier.predict(X_val)\n",
    "    auprc_score = average_precision_score(y_val, y_pred)\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    results[name] = {'AUPRC': auprc_score, 'Training Time (min)': round(training_time / 60, 2)}\n",
    "\n",
    "# Imprimir los resultados\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name} - AUPRC: {metrics['AUPRC']:.4f}, Training Time: {metrics['Training Time (min)']:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros: {'subsample': 0.8, 'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "AUPRC en el conjunto de validación: 0.9991209877901085\n",
      "AUPRC del modelo final en el conjunto de validación: 0.0044152489174309915\n",
      "Tiempo de entrenamiento: 75.09 minutos\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Definir el clasificador XGBoost\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Definir los hiperparámetros que deseamos ajustar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200, 250],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros con validación cruzada\n",
    "random_search = RandomizedSearchCV(estimator=xgb_model, param_distributions=param_grid, n_iter=6, scoring='average_precision', cv=3)\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train_resampled, y_train_resampled)\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Obtener los mejores hiperparámetros y el mejor resultado de AUPRC\n",
    "best_params = random_search.best_params_\n",
    "best_auprc = random_search.best_score_\n",
    "\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "print(\"AUPRC en el conjunto de validación:\", best_auprc)\n",
    "\n",
    "# Crear el modelo final con los mejores hiperparámetros\n",
    "final_xgb_model = XGBClassifier(random_state=42, **best_params)\n",
    "\n",
    "# Entrenar el modelo final en el conjunto de entrenamiento balanceado\n",
    "final_xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluar el modelo final en el conjunto de validación\n",
    "y_pred = final_xgb_model.predict(X_val)\n",
    "final_auprc = average_precision_score(y_val, y_pred)\n",
    "\n",
    "print(\"AUPRC del modelo final en el conjunto de validación:\", final_auprc)\n",
    "print(\"Tiempo de entrenamiento:\", round(training_time / 60, 2), \"minutos\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los tres algoritmos con mejor métrica son:\n",
    "\n",
    "**Gradient Boosting:**\n",
    "\n",
    "Gradient Boosting es un algoritmo de aprendizaje automático que se basa en la combinación de múltiples árboles de decisión más débiles para construir un modelo más fuerte. Cada árbol se construye secuencialmente para corregir los errores del árbol anterior. Esto se hace mediante el ajuste de los nuevos árboles a los residuos (diferencias) entre las predicciones del modelo actual y las etiquetas verdaderas. Al combinar varios árboles débiles, Gradient Boosting es capaz de lograr un buen rendimiento en una amplia variedad de problemas de aprendizaje supervisado.\n",
    "\n",
    "**Gaussian Naive Bayes:**\n",
    "\n",
    "Naive Bayes es un algoritmo de clasificación basado en el teorema de Bayes y la suposición de independencia condicional de las características. En el caso de Gaussian Naive Bayes, se asume que las características siguen una distribución gaussiana (normal). Aunque la suposición de independencia condicional puede no ser realista en muchos casos, el algoritmo sigue siendo efectivo en ciertos problemas, especialmente cuando hay muchas características y suficientes datos de entrenamiento. Es simple y rápido, y puede funcionar bien en problemas con características numéricas continuas.\n",
    "\n",
    "**XGBoost:**\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) es una implementación optimizada y eficiente del algoritmo Gradient Boosting. Utiliza técnicas de regularización, manejo de valores faltantes y paralelismo para mejorar el rendimiento y la generalización. XGBoost ha ganado mucha popularidad debido a su rendimiento sobresaliente en competiciones de aprendizaje automático y su capacidad para manejar grandes conjuntos de datos. Es altamente configurable y se puede ajustar mediante la búsqueda de hiperparámetros para obtener el mejor rendimiento en una variedad de problemas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
